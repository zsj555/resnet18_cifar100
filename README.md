# resnet18_cifar100
# 训练和测试步骤
## 1、下载数据集并做格式转换
在官网上下载数据集到本目录下 训练数据集大小为50000，测试数据集大小为10000
## 2、搭建resnet18网络结构
## 3、编写训练神经网络的函数
损失函数：交叉熵损失函数。CrossEntropyLoss 该损失函数结合了nn.LogSoftmax()和nn.NLLLoss()两个函数。
它在做分类（具体几类）训练的时候是非常有用的。在训练过程中，对于每个类分配权值，可选的参数权值应该是一个1D张量。当你有一个不平衡的训练集时，这是是非常有用的。
优化器：SGD： 基本梯度下降法
采用动量方法，将momentum设置为0.9，lr为学习率，weight_decay设置为正则项reg
## 4、优化超参数
### （1）确定大致学习率
由于在全部数据集上跑完训练过程所用时间过多，所以我们首先挑选了100 * batch_size（即100 * 32）张图片作为小样本,
然后首先设置三个学习率： 0.1，0.01，0.001，观察这三个学习率在小样本上面训练过程的loss以及acc曲线。
### （2）细化区间，继续寻找最优参数
根据上一步得到的较为合理的学习率lr=0.01，我们在其左右分别每隔0.002找到两个点，继续寻找最优超参数。
同时，需要寻找正则化项参数，reg分别有1e-5，1e-4，5e-4 即： 
lr=0.006，reg=1e-5； lr=0.006，reg=5e-4； lr=0.006，reg=1e-4； lr=0.008，reg=1e-5； lr=0.008，reg=5e-4； lr=0.008，reg=1e-4； lr=0.01， reg=1e-5； 
lr=0.01， reg=5e-4； lr=0.01， reg=1e-4； lr=0.012，reg=1e-5； lr=0.012，reg=5e-4； lr=0.012，reg=1e-4； lr=0.014，reg=1e-5； lr=0.014，reg=5e-4；
lr=0.014，reg=1e-4； 共计15对参数 同时我们选择减少训练epoch以减少训练时间，选择epoch=7
## 5、训练及测试
设置lr=0.01，reg=1e-4，学习率下降为0.95，步长epoch=12，开始训练，并检测在测试集上的准确率。保存模型。
## 6、数据增强
导入cutout.py,mixup.py,cutmix.py文件，分别在三种数据增强方法下训练并测试。
经比较发现，三种数据增强方法在一定程度上能够提升准确率，cutmix方法提升得更多。
## 7、提高准确率
先在lr=0.01，reg=1e-4下训练12epoch，保存模型，
接下来我们将保存的模型加载进来，降低学习速率继续学习（降低到 lr=0.001） ，固定前面的Conv2d层的参数，放开最后的fc层的参数反向学习功能，训练3epoch。
